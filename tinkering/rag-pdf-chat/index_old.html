<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chat with your PDF - Tinkering with AI</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #e8e8e8;
            color: #222222;
            line-height: 1.5;
            padding: 50px 20px 80px;
        }

        a { color: inherit; }

        .container {
            max-width: 700px;
            margin: 0 auto;
        }

        .back-link {
            display: inline-block;
            color: #666666;
            text-decoration: none;
            font-size: 12px;
            margin-bottom: 30px;
            transition: color 0.2s;
        }

        .back-link:hover {
            color: #000000;
        }

        .try-button {
            display: inline-block;
            padding: 10px 20px;
            background: #000000;
            color: #ffffff;
            text-decoration: none;
            border-radius: 6px;
            font-size: 13px;
            font-weight: 500;
            transition: all 0.2s;
            margin-bottom: 24px;
        }

        .try-button:hover {
            background: #1a1a1a;
            transform: translateY(-1px);
        }

        h1 {
            font-size: 36px;
            font-weight: 400;
            margin-bottom: 24px;
            line-height: 1.1;
            color: #000000;
        }

        h2 {
            font-size: 22px;
            font-weight: 400;
            margin-top: 36px;
            margin-bottom: 14px;
            color: #000000;
        }

        h3 {
            font-size: 16px;
            font-weight: 600;
            margin-top: 24px;
            margin-bottom: 10px;
            color: #000000;
        }

        h4 {
            font-size: 14px;
            font-weight: 600;
            margin-top: 18px;
            margin-bottom: 8px;
            color: #000000;
        }

        p {
            font-size: 14px;
            line-height: 1.5;
            margin-bottom: 14px;
            color: #222222;
        }

        ul, ol {
            margin-left: 18px;
            margin-bottom: 14px;
        }

        li {
            font-size: 14px;
            line-height: 1.5;
            margin-bottom: 5px;
            color: #222222;
        }

        strong {
            font-weight: 600;
        }

        code {
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            background: #f5f0e8;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 12px;
            color: #333333;
        }

        pre {
            background: #f5f0e8;
            border-radius: 8px;
            padding: 14px 16px;
            overflow-x: auto;
            margin: 14px 0;
            border: 1px solid #e8e0d5;
        }

        pre code {
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 11px;
            line-height: 1.4;
            color: #333333;
            background: none;
            padding: 0;
        }

        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 12px;
            margin: 14px 0;
        }

        .tech-card {
            background: #f5f0e8;
            border: 1px solid #e8e0d5;
            border-radius: 8px;
            padding: 12px 14px;
        }

        .tech-card h4 {
            font-size: 14px;
            font-weight: 600;
            margin: 0 0 6px 0;
            color: #000000;
        }

        .tech-card p {
            font-size: 13px;
            color: #666666;
            margin: 0;
        }

        @media (max-width: 768px) {
            body {
                padding: 40px 16px 60px;
            }

            h1 {
                font-size: 28px;
            }

            h2 {
                font-size: 20px;
            }

            p, li {
                font-size: 13px;
            }

            pre {
                padding: 12px 14px;
            }

            pre code {
                font-size: 10px;
            }

            .tech-stack {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <a class="back-link" href="/tinkering">← Back</a>

        <h1>Chat with your PDF</h1>

        <a href="https://ragpdf-umber.vercel.app/" target="_blank" class="try-button">Try It Now →</a>

        <p>A retrieval-augmented generation (RAG) system that lets you upload PDFs and ask questions with GPT-powered answers backed by document context.</p>

        <h2>The idea</h2>
        <p>Large language models are powerful, but they don't know about your specific documents. RAG solves this by retrieving relevant content from your documents and feeding it to the LLM as context.</p>

        <p>This project demonstrates a complete RAG workflow: upload a PDF, chunk and embed the text, store vectors in Pinecone, and query with GPT-4 using retrieved context for accurate, cited answers.</p>

        <h2>Architecture overview</h2>
        <p>The system consists of two main flows:</p>

        <h3>Ingestion pipeline</h3>
        <ol>
            <li><strong>PDF upload:</strong> User uploads a PDF through the frontend</li>
            <li><strong>Text extraction:</strong> Backend uses pdfplumber to extract text</li>
            <li><strong>Chunking:</strong> Text is split into 800-character chunks with 120-character overlap</li>
            <li><strong>Embedding:</strong> Each chunk is embedded using OpenAI's text-embedding-3-large (3,072 dimensions)</li>
            <li><strong>Storage:</strong> Vectors are stored in Pinecone with metadata (page number, document ID)</li>
        </ol>

        <h3>Query pipeline</h3>
        <ol>
            <li><strong>Question input:</strong> User asks a question via the chat interface</li>
            <li><strong>Query embedding:</strong> Question is embedded using the same model</li>
            <li><strong>Similarity search:</strong> Pinecone retrieves the 6 most relevant chunks</li>
            <li><strong>Context assembly:</strong> Retrieved chunks are formatted with metadata</li>
            <li><strong>GPT completion:</strong> GPT-4 generates an answer using the context</li>
            <li><strong>Citations:</strong> Response includes source page numbers and relevance scores</li>
        </ol>

        <h2>Tech stack</h2>

        <div class="tech-stack">
            <div class="tech-card">
                <h4>Backend</h4>
                <p>FastAPI, Python 3.9+, Uvicorn</p>
            </div>
            <div class="tech-card">
                <h4>Frontend</h4>
                <p>Vanilla JavaScript, HTML5, CSS3</p>
            </div>
            <div class="tech-card">
                <h4>Vector Database</h4>
                <p>Pinecone (serverless index)</p>
            </div>
            <div class="tech-card">
                <h4>AI Services</h4>
                <p>OpenAI GPT-4, text-embedding-3-large</p>
            </div>
            <div class="tech-card">
                <h4>PDF Processing</h4>
                <p>pdfplumber for text extraction</p>
            </div>
            <div class="tech-card">
                <h4>Deployment</h4>
                <p>Vercel (serverless functions)</p>
            </div>
        </div>

        <h2>Step 1: Initial setup</h2>

        <h3>Project structure</h3>
        <p>Created a clean separation between backend and frontend:</p>
        <ul>
            <li><code>backend/</code> - FastAPI service with routers for uploads and chat</li>
            <li><code>frontend/</code> - Static HTML/JS client</li>
            <li>Environment variables for API keys and configuration</li>
        </ul>

        <h3>Pinecone setup</h3>
        <p>Key configuration decisions:</p>
        <ul>
            <li>Index dimension: 3,072 (matches text-embedding-3-large)</li>
            <li>Metric: cosine similarity</li>
            <li>Namespace per document for isolation</li>
        </ul>

        <h2>Step 2: Backend API</h2>

        <h3>PDF upload endpoint</h3>
        <p>Built <code>/api/uploads/pdf</code> to handle the full ingestion pipeline:</p>
        <ul>
            <li>Accept multipart file upload (max 25MB)</li>
            <li>Extract text with pdfplumber (preserves page numbers)</li>
            <li>Chunk text with configurable size and overlap</li>
            <li>Generate embeddings in batches for efficiency</li>
            <li>Upsert to Pinecone with metadata</li>
        </ul>

        <h3>Q&A endpoint</h3>
        <p>Created <code>/api/chat/qa</code> for the retrieval-augmented generation:</p>
        <ul>
            <li>Embed the user's question</li>
            <li>Query Pinecone across specified document namespaces</li>
            <li>Format context with page numbers for GPT</li>
            <li>Use GPT-4 to synthesize an answer</li>
            <li>Return answer with citations</li>
        </ul>

        <h2>Step 3: Vector embeddings</h2>

        <h3>Chunking strategy</h3>
        <p>Implemented overlapping chunks to maintain context:</p>
        <ul>
            <li>800 characters per chunk (roughly 150-200 tokens)</li>
            <li>120-character overlap to preserve sentence boundaries</li>
            <li>Each chunk retains page number metadata</li>
        </ul>

        <h3>Embedding model choice</h3>
        <p>Chose OpenAI's text-embedding-3-large for:</p>
        <ul>
            <li>High dimensional representation (3,072 dimensions)</li>
            <li>Strong semantic understanding</li>
            <li>Good balance of quality and speed</li>
        </ul>

        <h3>Pinecone indexing</h3>
        <p>Used document-specific namespaces:</p>
        <ul>
            <li>Each PDF gets a unique namespace based on its ID</li>
            <li>Allows querying specific documents or all documents</li>
            <li>Metadata includes page number and relevance score</li>
        </ul>

        <h2>Step 4: Frontend UI</h2>

        <h3>Drag-and-drop upload</h3>
        <p>Built an intuitive file upload experience:</p>
        <ul>
            <li>Drag-and-drop zone with visual feedback</li>
            <li>File type validation (PDFs only)</li>
            <li>Progress indicators during upload and processing</li>
            <li>Success/error messaging</li>
        </ul>

        <h3>Chat interface</h3>
        <p>Simple, clean chat UI with:</p>
        <ul>
            <li>Message history display</li>
            <li>User and assistant message differentiation</li>
            <li>Citations displayed below each answer</li>
            <li>Source page numbers with relevance scores</li>
        </ul>

        <h3>API integration</h3>
        <p>Frontend communicates with backend via:</p>
        <ul>
            <li>FormData for PDF uploads</li>
            <li>JSON for Q&A requests</li>
            <li>Environment variable for API base URL (supports local and deployed)</li>
        </ul>

        <h2>Challenges encountered</h2>

        <h3>1. Chunking granularity</h3>
        <p>Finding the right chunk size was tricky. Too small and you lose context; too large and embeddings are less precise. Settled on 800 characters with 120-character overlap after testing.</p>

        <h3>2. PDF text extraction</h3>
        <p>PDFs with complex layouts (tables, multi-column) sometimes extract text in the wrong order. pdfplumber works well for most documents but requires page-by-page extraction for better control.</p>

        <h3>3. Context window management</h3>
        <p>GPT-4 has a large context window, but fitting 6 chunks plus the question comfortably required careful formatting. Limited to 6 chunks (max ~5,000 tokens of context) to leave room for the response.</p>

        <h3>4. Citation accuracy</h3>
        <p>Ensuring GPT cites the correct page numbers required explicit instructions in the system prompt. Added metadata to each chunk so the model knows which page it came from.</p>

        <h2>Key takeaways</h2>

        <h3>What worked well</h3>
        <ul>
            <li><strong>FastAPI:</strong> Clean, fast, and easy to deploy as serverless functions</li>
            <li><strong>Pinecone:</strong> Serverless vector database with excellent performance and simple API</li>
            <li><strong>OpenAI embeddings:</strong> High-quality semantic search with minimal tuning</li>
            <li><strong>Overlapping chunks:</strong> Prevented information loss at chunk boundaries</li>
            <li><strong>Namespaces:</strong> Clean way to isolate documents while allowing multi-document search</li>
        </ul>

        <h3>What I'd do differently</h3>
        <ul>
            <li><strong>Semantic chunking:</strong> Use sentence-based or semantic chunking instead of character-based</li>
            <li><strong>Metadata filtering:</strong> Add more metadata (section headers, doc type) for better filtering</li>
            <li><strong>Streaming responses:</strong> Stream GPT responses for faster perceived performance</li>
            <li><strong>Chat history:</strong> Persist conversation history for follow-up questions</li>
            <li><strong>Hybrid search:</strong> Combine vector search with keyword search for better recall</li>
        </ul>

        <h3>Cost considerations</h3>
        <p>Per-document costs:</p>
        <ul>
            <li>Embedding (10-page PDF): ~$0.01</li>
            <li>Storage in Pinecone: ~$0.001/month</li>
            <li>Query + GPT-4 response: ~$0.02</li>
            <li><strong>Total: ~$0.03 per document + $0.02 per query</strong></li>
        </ul>

        <h3>Next steps</h3>
        <ul>
            <li>Add user authentication and per-user document management</li>
            <li>Store PDF binaries in S3 for later retrieval</li>
            <li>Implement semantic caching to reduce costs on repeated questions</li>
            <li>Add support for other file types (DOCX, TXT, Markdown)</li>
            <li>Build a document management UI to browse and delete uploads</li>
        </ul>
    </div>
</body>
</html>
