<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building RAG PDF Chat - Tinkering with AI</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        :root {
            color-scheme: light;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;
            background: #ffffff;
            color: #0f172a;
            line-height: 1.7;
        }

        a {
            color: inherit;
        }

        .top-nav {
            position: sticky;
            top: 0;
            z-index: 20;
            background: #ffffff;
            border-bottom: 1px solid #e2e8f0;
        }

        .top-nav-inner {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 18px 32px;
        }

        .top-link {
            color: #64748b;
            text-decoration: none;
            padding-bottom: 4px;
            border-bottom: 2px solid transparent;
            transition: color 0.2s, border-color 0.2s;
        }

        .top-link:hover {
            color: #111827;
        }

        .doc-layout {
            max-width: 1200px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 260px minmax(0, 1fr);
            gap: 48px;
            padding: 32px 32px 120px;
        }

        .sidebar {
            position: sticky;
            top: 92px;
            align-self: start;
            height: calc(100vh - 120px);
            overflow-y: auto;
            padding-right: 16px;
            border-right: 1px solid #e2e8f0;
        }

        .sidebar::-webkit-scrollbar {
            width: 6px;
        }

        .sidebar::-webkit-scrollbar-thumb {
            background: rgba(148, 163, 184, 0.4);
            border-radius: 999px;
        }

        .sidebar-heading {
            font-size: 12px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.08em;
            color: #94a3b8;
            margin-bottom: 12px;
        }

        .nav-group {
            margin-bottom: 32px;
        }

        .sidebar-link {
            display: block;
            text-decoration: none;
            color: #334155;
            font-size: 14px;
            padding: 8px 0 8px 14px;
            border-left: 2px solid transparent;
            border-radius: 0 10px 10px 0;
            transition: color 0.2s, border-color 0.2s, background 0.2s;
        }

        .sidebar-link:hover {
            color: #0f172a;
        }

        .sidebar-link.active {
            color: #0f172a;
            font-weight: 600;
            border-color: #10b981;
            background: #d1fae5;
        }

        .article {
            max-width: 740px;
        }

        .article h1 {
            font-size: 36px;
            font-weight: 600;
            margin-bottom: 20px;
            line-height: 1.2;
            color: #111827;
        }

        .article > p {
            font-size: 18px;
            color: #334155;
            margin-bottom: 32px;
        }

        section {
            margin-bottom: 56px;
            scroll-margin-top: 100px;
        }

        section:last-of-type {
            margin-bottom: 0;
        }

        section h2 {
            font-size: 26px;
            font-weight: 600;
            margin-bottom: 16px;
            color: #0f172a;
        }

        section h3 {
            font-size: 18px;
            font-weight: 600;
            margin: 28px 0 12px;
            color: #111827;
        }

        p {
            margin-bottom: 18px;
            color: #334155;
        }

        ul, ol {
            margin: 0 0 24px 24px;
            color: #334155;
        }

        li {
            margin-bottom: 12px;
        }

        code {
            font-family: 'SFMono-Regular', Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 6px;
            font-size: 13px;
        }

        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 16px;
            margin: 24px 0;
        }

        .tech-card {
            border: 1px solid #e2e8f0;
            border-radius: 12px;
            padding: 16px;
            background: #f8fafc;
        }

        .tech-card h4 {
            font-size: 14px;
            font-weight: 600;
            margin-bottom: 8px;
            color: #0f172a;
        }

        .tech-card p {
            font-size: 13px;
            color: #64748b;
            margin: 0;
        }

        @media (max-width: 1024px) {
            .top-nav-inner,
            .doc-layout {
                padding-left: 20px;
                padding-right: 20px;
            }
        }

        @media (max-width: 960px) {
            .doc-layout {
                grid-template-columns: 1fr;
                gap: 0;
            }

            .sidebar {
                position: static;
                height: auto;
                border-right: none;
                border-bottom: 1px solid #e2e8f0;
                padding-bottom: 20px;
                margin-bottom: 32px;
            }
        }
    </style>
</head>
<body>
    <header class="top-nav">
        <div class="top-nav-inner">
            <a class="top-link active" href="tinkering.html" style="border-bottom: none; font-weight: 500;">&larr; Back</a>
        </div>
    </header>

    <div class="doc-layout">
        <aside class="sidebar">
            <div class="nav-group">
                <div class="sidebar-heading">Overview</div>
                <a class="sidebar-link" href="#idea">The idea</a>
                <a class="sidebar-link" href="#architecture">Architecture</a>
                <a class="sidebar-link" href="#tech-stack">Tech stack</a>
            </div>

            <div class="nav-group">
                <div class="sidebar-heading">Build Process</div>
                <a class="sidebar-link" href="#setup">Initial setup</a>
                <a class="sidebar-link" href="#backend">Backend API</a>
                <a class="sidebar-link" href="#embeddings">Vector embeddings</a>
                <a class="sidebar-link" href="#frontend">Frontend UI</a>
            </div>

            <div class="nav-group">
                <div class="sidebar-heading">Lessons Learned</div>
                <a class="sidebar-link" href="#challenges">Challenges</a>
                <a class="sidebar-link" href="#takeaways">Key takeaways</a>
            </div>
        </aside>

        <article class="article">
            <section id="idea">
                <h1>Building RAG PDF Chat</h1>
                <p>A retrieval-augmented generation (RAG) system that lets you upload PDFs and ask questions with GPT-powered answers backed by document context.</p>

                <h2>The idea</h2>
                <p>Large language models are powerful, but they don't know about your specific documents. RAG solves this by retrieving relevant content from your documents and feeding it to the LLM as context.</p>
                
                <p>This project demonstrates a complete RAG workflow: upload a PDF, chunk and embed the text, store vectors in Pinecone, and query with GPT-4 using retrieved context for accurate, cited answers.</p>
            </section>

            <section id="architecture">
                <h2>Architecture overview</h2>
                <p>The system consists of two main flows:</p>
                
                <h3>Ingestion pipeline</h3>
                <ol>
                    <li><strong>PDF upload:</strong> User uploads a PDF through the frontend</li>
                    <li><strong>Text extraction:</strong> Backend uses pdfplumber to extract text</li>
                    <li><strong>Chunking:</strong> Text is split into 800-character chunks with 120-character overlap</li>
                    <li><strong>Embedding:</strong> Each chunk is embedded using OpenAI's text-embedding-3-large (3,072 dimensions)</li>
                    <li><strong>Storage:</strong> Vectors are stored in Pinecone with metadata (page number, document ID)</li>
                </ol>

                <h3>Query pipeline</h3>
                <ol>
                    <li><strong>Question input:</strong> User asks a question via the chat interface</li>
                    <li><strong>Query embedding:</strong> Question is embedded using the same model</li>
                    <li><strong>Similarity search:</strong> Pinecone retrieves the 6 most relevant chunks</li>
                    <li><strong>Context assembly:</strong> Retrieved chunks are formatted with metadata</li>
                    <li><strong>GPT completion:</strong> GPT-4 generates an answer using the context</li>
                    <li><strong>Citations:</strong> Response includes source page numbers and relevance scores</li>
                </ol>
            </section>

            <section id="tech-stack">
                <h2>Tech stack</h2>
                
                <div class="tech-stack">
                    <div class="tech-card">
                        <h4>Backend</h4>
                        <p>FastAPI, Python 3.9+, Uvicorn</p>
                    </div>
                    <div class="tech-card">
                        <h4>Frontend</h4>
                        <p>Vanilla JavaScript, HTML5, CSS3</p>
                    </div>
                    <div class="tech-card">
                        <h4>Vector Database</h4>
                        <p>Pinecone (serverless index)</p>
                    </div>
                    <div class="tech-card">
                        <h4>AI Services</h4>
                        <p>OpenAI GPT-4, text-embedding-3-large</p>
                    </div>
                    <div class="tech-card">
                        <h4>PDF Processing</h4>
                        <p>pdfplumber for text extraction</p>
                    </div>
                    <div class="tech-card">
                        <h4>Deployment</h4>
                        <p>Vercel (serverless functions)</p>
                    </div>
                </div>
            </section>

            <section id="setup">
                <h2>Step 1: Initial setup</h2>
                
                <h3>Project structure</h3>
                <p>Created a clean separation between backend and frontend:</p>
                <ul>
                    <li><code>backend/</code> - FastAPI service with routers for uploads and chat</li>
                    <li><code>frontend/</code> - Static HTML/JS client</li>
                    <li>Environment variables for API keys and configuration</li>
                </ul>

                <h3>Pinecone setup</h3>
                <p>Key configuration decisions:</p>
                <ul>
                    <li>Index dimension: 3,072 (matches text-embedding-3-large)</li>
                    <li>Metric: cosine similarity</li>
                    <li>Namespace per document for isolation</li>
                </ul>
            </section>

            <section id="backend">
                <h2>Step 2: Backend API</h2>
                
                <h3>PDF upload endpoint</h3>
                <p>Built <code>/api/uploads/pdf</code> to handle the full ingestion pipeline:</p>
                <ul>
                    <li>Accept multipart file upload (max 25MB)</li>
                    <li>Extract text with pdfplumber (preserves page numbers)</li>
                    <li>Chunk text with configurable size and overlap</li>
                    <li>Generate embeddings in batches for efficiency</li>
                    <li>Upsert to Pinecone with metadata</li>
                </ul>

                <h3>Q&A endpoint</h3>
                <p>Created <code>/api/chat/qa</code> for the retrieval-augmented generation:</p>
                <ul>
                    <li>Embed the user's question</li>
                    <li>Query Pinecone across specified document namespaces</li>
                    <li>Format context with page numbers for GPT</li>
                    <li>Use GPT-4 to synthesize an answer</li>
                    <li>Return answer with citations</li>
                </ul>
            </section>

            <section id="embeddings">
                <h2>Step 3: Vector embeddings</h2>
                
                <h3>Chunking strategy</h3>
                <p>Implemented overlapping chunks to maintain context:</p>
                <ul>
                    <li>800 characters per chunk (roughly 150-200 tokens)</li>
                    <li>120-character overlap to preserve sentence boundaries</li>
                    <li>Each chunk retains page number metadata</li>
                </ul>

                <h3>Embedding model choice</h3>
                <p>Chose OpenAI's text-embedding-3-large for:</p>
                <ul>
                    <li>High dimensional representation (3,072 dimensions)</li>
                    <li>Strong semantic understanding</li>
                    <li>Good balance of quality and speed</li>
                </ul>

                <h3>Pinecone indexing</h3>
                <p>Used document-specific namespaces:</p>
                <ul>
                    <li>Each PDF gets a unique namespace based on its ID</li>
                    <li>Allows querying specific documents or all documents</li>
                    <li>Metadata includes page number and relevance score</li>
                </ul>
            </section>

            <section id="frontend">
                <h2>Step 4: Frontend UI</h2>
                
                <h3>Drag-and-drop upload</h3>
                <p>Built an intuitive file upload experience:</p>
                <ul>
                    <li>Drag-and-drop zone with visual feedback</li>
                    <li>File type validation (PDFs only)</li>
                    <li>Progress indicators during upload and processing</li>
                    <li>Success/error messaging</li>
                </ul>

                <h3>Chat interface</h3>
                <p>Simple, clean chat UI with:</p>
                <ul>
                    <li>Message history display</li>
                    <li>User and assistant message differentiation</li>
                    <li>Citations displayed below each answer</li>
                    <li>Source page numbers with relevance scores</li>
                </ul>

                <h3>API integration</h3>
                <p>Frontend communicates with backend via:</p>
                <ul>
                    <li>FormData for PDF uploads</li>
                    <li>JSON for Q&A requests</li>
                    <li>Environment variable for API base URL (supports local and deployed)</li>
                </ul>
            </section>

            <section id="challenges">
                <h2>Challenges encountered</h2>
                
                <h3>1. Chunking granularity</h3>
                <p>Finding the right chunk size was tricky. Too small and you lose context; too large and embeddings are less precise. Settled on 800 characters with 120-character overlap after testing.</p>

                <h3>2. PDF text extraction</h3>
                <p>PDFs with complex layouts (tables, multi-column) sometimes extract text in the wrong order. pdfplumber works well for most documents but requires page-by-page extraction for better control.</p>

                <h3>3. Context window management</h3>
                <p>GPT-4 has a large context window, but fitting 6 chunks plus the question comfortably required careful formatting. Limited to 6 chunks (max ~5,000 tokens of context) to leave room for the response.</p>

                <h3>4. Citation accuracy</h3>
                <p>Ensuring GPT cites the correct page numbers required explicit instructions in the system prompt. Added metadata to each chunk so the model knows which page it came from.</p>
            </section>

            <section id="takeaways">
                <h2>Key takeaways</h2>
                
                <h3>What worked well</h3>
                <ul>
                    <li><strong>FastAPI:</strong> Clean, fast, and easy to deploy as serverless functions</li>
                    <li><strong>Pinecone:</strong> Serverless vector database with excellent performance and simple API</li>
                    <li><strong>OpenAI embeddings:</strong> High-quality semantic search with minimal tuning</li>
                    <li><strong>Overlapping chunks:</strong> Prevented information loss at chunk boundaries</li>
                    <li><strong>Namespaces:</strong> Clean way to isolate documents while allowing multi-document search</li>
                </ul>

                <h3>What I'd do differently</h3>
                <ul>
                    <li><strong>Semantic chunking:</strong> Use sentence-based or semantic chunking instead of character-based</li>
                    <li><strong>Metadata filtering:</strong> Add more metadata (section headers, doc type) for better filtering</li>
                    <li><strong>Streaming responses:</strong> Stream GPT responses for faster perceived performance</li>
                    <li><strong>Chat history:</strong> Persist conversation history for follow-up questions</li>
                    <li><strong>Hybrid search:</strong> Combine vector search with keyword search for better recall</li>
                </ul>

                <h3>Cost considerations</h3>
                <p>Per-document costs:</p>
                <ul>
                    <li>Embedding (10-page PDF): ~$0.01</li>
                    <li>Storage in Pinecone: ~$0.001/month</li>
                    <li>Query + GPT-4 response: ~$0.02</li>
                    <li><strong>Total: ~$0.03 per document + $0.02 per query</strong></li>
                </ul>

                <h3>Next steps</h3>
                <ul>
                    <li>Add user authentication and per-user document management</li>
                    <li>Store PDF binaries in S3 for later retrieval</li>
                    <li>Implement semantic caching to reduce costs on repeated questions</li>
                    <li>Add support for other file types (DOCX, TXT, Markdown)</li>
                    <li>Build a document management UI to browse and delete uploads</li>
                </ul>
            </section>
        </article>
    </div>

    <script>
        const navLinks = Array.from(document.querySelectorAll('.sidebar-link'));
        const sections = navLinks.map(link => document.querySelector(link.getAttribute('href'))).filter(Boolean);

        if (navLinks.length && sections.length) {
            navLinks[0].classList.add('active');

            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (!entry.isIntersecting) {
                        return;
                    }
                    const index = sections.indexOf(entry.target);
                    if (index === -1) {
                        return;
                    }
                    navLinks.forEach(link => link.classList.remove('active'));
                    navLinks[index].classList.add('active');
                });
            }, {
                rootMargin: '-50% 0px -50% 0px',
                threshold: 0
            });

            sections.forEach(section => observer.observe(section));
        }
    </script>
</body>
</html>
